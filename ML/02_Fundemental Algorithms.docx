Algorithms for Machine Learning

1. Linear Regression
	Through linear regression, we develop a linear model from the data, and use it to perform prediction.

The basic step of performing regression is
    • Define a model
    • Define loss function – optimisation function
    • Optimise the model using the loss function using any optimisation algorithm
    • Evaluate result


There are two types of linear regression:

1. Simple linear regression:

Uses f(x) = w.x + b, where w and b are real number values.
There is only one feature variable x to create the model, and the prediction y. The model ends up being a line in 2-dimensional plane.

2. Multiple linear regression

	In multiple linear regression, there are multiple feature 
variables and their corresponding weights. If there are n feature
variables, the model will be a n+1 dimensional hyperplane in an 
n+1 dimensional space, with one dimension referring to the 
prediction value f(x).

The model is represented as 
		f(x) = w1.x1 + w2.x2 + . . . +wn.xn +b
or as f(x) = w. x + b, 
where w and b are n-dimensional vectors and . Represents dot product between w and x.



1) Defining model
	
The equation of linear model, used in simple linear regression is  
					f(x) = w.x + b

We will use some optimisation algorithm (like SGD, OLS, etc.) to 
find the optimal values of w(weights) and b(biases), denoted as w* 
and b*, by defining a loss function, and trying to minimise it.

2) Loss function
	The loss function is the function which is being optimised(minimised) inorder to get the optimal values w* and b*. For linear regression, the most common loss or cost function is the Mean Squared Error (MSE). The MSE is a measure of the average squared difference between the predicted values and the actual target values for all the data points in your dataset. It's calculated as:

				  MSE = (1/N) * Σ [ f(x) – y ]2

This is the objective function. Here the loss function is (f(x) – y)2. The cost function is a term often used interchangeably with the loss function. The distinction between them can be subtle but generally, the loss function computes the error for a single data point, while the cost function calculates the overall error for the entire dataset, often as the average or sum of the individual losses.

The reason why we use squared error loss function is:
    • The loss have to be positive. 
    • The loss function should be easily derivable
    • The graph should be smooth for GD to converge fast

3) Optimisation
	One common optimisation method used for minimising the objective function is Gradient Descent.

4) Evaluation
	Involves usingng accuracy, precision, recall, etc. to measure the accuracy of the model.


2. Polynomial Regression

	Polynomial regression is a variation of linear regression that involves using polynomial functions to model the relationship between the feature variable(s) and the target variable. While the basic idea is similar to linear regression, the model is extended to capture non-linear relationships.

			f(x) = w1.x + w2.x2 + . . . +wn.xn + b 

Higher-degree polynomials can fit more complex curves but might also lead to overfitting.The term "linear" in linear regression and polynomial regression refers to the linearity in the parameters (coefficients), not necessarily the relationship between variables.

3.Logistic Regression
	Logistic regression is used to perform classification [ not regression  (-_-)].
